"""
ü§ñ M√ìDULO 2: LLMs B√ÅSICOS
=========================

En este m√≥dulo profundizar√°s en el uso de Large Language Models con LangChain.
Aprender√°s sobre diferentes modelos, configuraciones avanzadas y mejores pr√°cticas.

üéØ OBJETIVOS DE APRENDIZAJE:
- Entender los diferentes tipos de LLMs disponibles
- Configurar par√°metros avanzados de LLMs
- Manejar errores y excepciones
- Optimizar el rendimiento y costos
- Trabajar con diferentes proveedores de LLMs

üìö CONCEPTOS CLAVE:
- Chat Models vs Completion Models
- Par√°metros de configuraci√≥n (temperature, max_tokens, etc.)
- Manejo de errores y rate limiting
- Costos y optimizaci√≥n
- Modelos de diferentes proveedores

Autor: Tu Instructor de LangChain
Fecha: 2024
"""

import sys
import os
import time
from typing import List, Dict, Any

# Agregar el directorio ra√≠z al path para importar utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.config import config, print_config_status
from utils.helpers import print_separator, print_step, print_result, measure_execution_time

# Importaciones de LangChain
from langchain_openai import ChatOpenAI, OpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.callbacks import get_openai_callback
import openai

def tipos_llms():
    """
    üîç Diferentes tipos de LLMs en LangChain
    """
    print_separator("TIPOS DE LLMs")
    
    print("""
    ü§ñ LangChain soporta diferentes tipos de LLMs:
    
    1. üìù Chat Models (Modelos de Chat):
       - Dise√±ados para conversaciones
       - Aceptan mensajes estructurados
       - Ejemplos: GPT-3.5-turbo, GPT-4, Claude
       - Mejor para aplicaciones conversacionales
    
    2. üîÑ Completion Models (Modelos de Completaci√≥n):
       - Dise√±ados para completar texto
       - Aceptan texto simple
       - Ejemplos: text-davinci-003, text-curie-001
       - Mejor para generaci√≥n de texto
    
    3. üåê Diferentes Proveedores:
       - OpenAI (GPT-3.5, GPT-4)
       - Anthropic (Claude)
       - Google (PaLM, Gemini)
       - Hugging Face (modelos locales)
       - Cohere
    """)

def chat_models_basicos():
    """
    üí¨ Trabajando con Chat Models
    """
    print_step(1, "Chat Models B√°sicos", "Creando y configurando modelos de chat")
    
    try:
        # Modelo b√°sico
        chat_model = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=150
        )
        
        print_result("Chat Model B√°sico", chat_model)
        
        # Mensajes estructurados
        mensajes = [
            SystemMessage(content="Eres un asistente experto en programaci√≥n Python."),
            HumanMessage(content="¬øQu√© es una funci√≥n lambda?")
        ]
        
        print(f"\nüì§ Enviando mensajes estructurados...")
        respuesta = chat_model.invoke(mensajes)
        
        print(f"üì• Respuesta: {respuesta.content}")
        
        return chat_model
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def completion_models():
    """
    üîÑ Trabajando con Completion Models
    """
    print_step(2, "Completion Models", "Usando modelos de completaci√≥n")
    
    try:
        # Modelo de completaci√≥n
        completion_model = OpenAI(
            model="text-davinci-003",
            temperature=0.5,
            max_tokens=100
        )
        
        print_result("Completion Model", completion_model)
        
        # Texto simple
        prompt = "Explica qu√© es una API REST en una frase:"
        
        print(f"\nüì§ Enviando prompt: {prompt}")
        respuesta = completion_model.invoke(prompt)
        
        print(f"üì• Respuesta: {respuesta}")
        
        return completion_model
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def parametros_avanzados():
    """
    ‚öôÔ∏è Configuraci√≥n avanzada de par√°metros
    """
    print_step(3, "Par√°metros Avanzados", "Explorando opciones de configuraci√≥n")
    
    parametros = {
        "temperature": {
            "descripcion": "Controla la aleatoriedad (0-1)",
            "valores": [0.1, 0.5, 0.9],
            "efecto": "0=determin√≠stico, 1=muy creativo"
        },
        "max_tokens": {
            "descripcion": "M√°ximo n√∫mero de tokens en respuesta",
            "valores": [50, 100, 200],
            "efecto": "Controla la longitud de la respuesta"
        },
        "top_p": {
            "descripcion": "N√∫cleo de probabilidad (0-1)",
            "valores": [0.1, 0.5, 0.9],
            "efecto": "Controla la diversidad del vocabulario"
        },
        "frequency_penalty": {
            "descripcion": "Penalizaci√≥n por frecuencia (-2 a 2)",
            "valores": [-1, 0, 1],
            "efecto": "Reduce repetici√≥n de palabras"
        },
        "presence_penalty": {
            "descripcion": "Penalizaci√≥n por presencia (-2 a 2)",
            "valores": [-1, 0, 1],
            "efecto": "Reduce repetici√≥n de temas"
        }
    }
    
    print("\n‚öôÔ∏è PAR√ÅMETROS DE CONFIGURACI√ìN:")
    for param, info in parametros.items():
        print(f"\nüîπ {param.upper()}:")
        print(f"   Descripci√≥n: {info['descripcion']}")
        print(f"   Valores t√≠picos: {info['valores']}")
        print(f"   Efecto: {info['efecto']}")

def experimentar_temperature():
    """
    üå°Ô∏è Experimentando con diferentes temperaturas
    """
    print_step(4, "Experimento: Temperature", "Viendo c√≥mo afecta la creatividad")
    
    try:
        prompt = "Escribe una historia corta sobre un robot que aprende a programar."
        
        temperaturas = [0.1, 0.5, 0.9]
        
        for temp in temperaturas:
            print(f"\nüå°Ô∏è Temperature: {temp}")
            print("-" * 30)
            
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=temp,
                max_tokens=100
            )
            
            respuesta = llm.invoke([HumanMessage(content=prompt)])
            print(f"Respuesta: {respuesta.content[:150]}...")
            
            time.sleep(1)  # Pausa para evitar rate limiting
            
    except Exception as e:
        print(f"‚ùå Error en experimento: {e}")

def manejo_errores():
    """
    üõ°Ô∏è Manejo de errores y excepciones
    """
    print_step(5, "Manejo de Errores", "Aprendiendo a manejar problemas comunes")
    
    errores_comunes = {
        "Rate Limiting": "Demasiadas solicitudes en poco tiempo",
        "API Key Inv√°lida": "Clave de API incorrecta o expirada",
        "Modelo No Disponible": "Modelo no existe o no est√° disponible",
        "Tokens Excedidos": "Respuesta m√°s larga que max_tokens",
        "Timeout": "La solicitud tard√≥ demasiado"
    }
    
    print("\nüõ°Ô∏è ERRORES COMUNES Y SOLUCIONES:")
    for error, descripcion in errores_comunes.items():
        print(f"   üîπ {error}: {descripcion}")
    
    # Ejemplo de manejo de errores
    try:
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=50,
            request_timeout=10  # Timeout de 10 segundos
        )
        
        respuesta = llm.invoke([HumanMessage(content="Hola")])
        print(f"\n‚úÖ Solicitud exitosa: {respuesta.content}")
        
    except openai.RateLimitError:
        print("‚ùå Rate limit alcanzado. Espera un momento.")
    except openai.AuthenticationError:
        print("‚ùå Error de autenticaci√≥n. Verifica tu API key.")
    except openai.APITimeoutError:
        print("‚ùå Timeout. La solicitud tard√≥ demasiado.")
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")

def optimizacion_costos():
    """
    üí∞ Optimizaci√≥n de costos
    """
    print_step(6, "Optimizaci√≥n de Costos", "Maximizando eficiencia y minimizando gastos")
    
    print("""
    üí∞ ESTRATEGIAS DE OPTIMIZACI√ìN:
    
    1. üìè Control de Longitud:
       - Usa max_tokens apropiado
       - Limita el contexto de entrada
       - Evita prompts innecesariamente largos
    
    2. üéØ Modelos Eficientes:
       - GPT-3.5-turbo es m√°s barato que GPT-4
       - Usa modelos m√°s peque√±os para tareas simples
       - Considera modelos locales para desarrollo
    
    3. üîÑ Caching:
       - Guarda respuestas frecuentes
       - Reutiliza resultados similares
       - Implementa cache local
    
    4. üìä Monitoreo:
       - Usa callbacks para tracking
       - Monitorea uso de tokens
       - Establece l√≠mites de gasto
    """)
    
    # Ejemplo con callback para monitoreo
    try:
        with get_openai_callback() as cb:
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
            respuesta = llm.invoke([HumanMessage(content="Explica qu√© es Python en una frase.")])
            
            print(f"\nüìä ESTAD√çSTICAS DE USO:")
            print(f"   Tokens totales: {cb.total_tokens}")
            print(f"   Tokens de prompt: {cb.prompt_tokens}")
            print(f"   Tokens de respuesta: {cb.completion_tokens}")
            print(f"   Costo total: ${cb.total_cost}")
            
    except Exception as e:
        print(f"‚ùå Error en monitoreo: {e}")

def diferentes_proveedores():
    """
    üåê Trabajando con diferentes proveedores
    """
    print_step(7, "Diferentes Proveedores", "Explorando opciones de LLMs")
    
    proveedores = {
        "OpenAI": {
            "modelos": ["gpt-3.5-turbo", "gpt-4", "text-davinci-003"],
            "ventajas": "Muy estable, buena documentaci√≥n",
            "desventajas": "Puede ser costoso"
        },
        "Anthropic": {
            "modelos": ["claude-2", "claude-instant"],
            "ventajas": "Muy bueno para an√°lisis",
            "desventajas": "API m√°s nueva"
        },
        "Google": {
            "modelos": ["text-bison", "chat-bison"],
            "ventajas": "Integraci√≥n con Google Cloud",
            "desventajas": "Menos documentaci√≥n"
        },
        "Hugging Face": {
            "modelos": ["miles de modelos locales"],
            "ventajas": "Gratis, privado",
            "desventajas": "Requiere m√°s recursos"
        }
    }
    
    print("\nüåê PROVEEDORES DE LLMs:")
    for proveedor, info in proveedores.items():
        print(f"\nüîπ {proveedor}:")
        print(f"   Modelos: {', '.join(info['modelos'])}")
        print(f"   Ventajas: {info['ventajas']}")
        print(f"   Desventajas: {info['desventajas']}")

def ejercicios_avanzados():
    """
    üéØ Ejercicios avanzados para pr√°ctica
    """
    print_step(8, "Ejercicios Avanzados", "Pon en pr√°ctica lo aprendido")
    
    ejercicios = [
        {
            "titulo": "Comparador de Modelos",
            "descripcion": "Crea una funci√≥n que compare respuestas de diferentes modelos",
            "objetivo": "Entender diferencias entre modelos"
        },
        {
            "titulo": "Optimizador de Prompts",
            "descripcion": "Crea prompts que minimicen tokens pero maximicen calidad",
            "objetivo": "Aprender optimizaci√≥n de costos"
        },
        {
            "titulo": "Manejador de Errores",
            "descripcion": "Implementa un sistema robusto de manejo de errores",
            "objetivo": "Crear aplicaciones robustas"
        },
        {
            "titulo": "Monitor de Costos",
            "descripcion": "Crea un sistema que monitoree y limite gastos",
            "objetivo": "Control de presupuesto"
        }
    ]
    
    print("\nüéØ EJERCICIOS AVANZADOS:")
    for i, ejercicio in enumerate(ejercicios, 1):
        print(f"\n{i}. {ejercicio['titulo']}")
        print(f"   Descripci√≥n: {ejercicio['descripcion']}")
        print(f"   Objetivo: {ejercicio['objetivo']}")

def mejores_practicas():
    """
    ‚≠ê Mejores pr√°cticas para LLMs
    """
    print_step(9, "Mejores Pr√°cticas", "Consejos para usar LLMs efectivamente")
    
    practicas = [
        "üéØ Siempre especifica el contexto y rol del modelo",
        "üìè Usa max_tokens apropiado para tu caso de uso",
        "üå°Ô∏è Ajusta temperature seg√∫n la tarea (baja para hechos, alta para creatividad)",
        "üõ°Ô∏è Implementa manejo de errores robusto",
        "üí∞ Monitorea costos y optimiza cuando sea posible",
        "üîÑ Usa caching para respuestas frecuentes",
        "üìä Implementa logging para debugging",
        "‚ö° Considera modelos m√°s peque√±os para tareas simples",
        "üîí Maneja API keys de forma segura",
        "üìö Documenta tus prompts y configuraciones"
    ]
    
    print("\n‚≠ê MEJORES PR√ÅCTICAS:")
    for practica in practicas:
        print(f"   {practica}")

def main():
    """
    üéØ Funci√≥n principal del m√≥dulo
    """
    print_separator("M√ìDULO 2: LLMs B√ÅSICOS")
    
    # Verificar configuraci√≥n
    if not config.validate_config():
        return
    
    # Contenido del m√≥dulo
    tipos_llms()
    
    # Ejemplos pr√°cticos
    chat_models_basicos()
    completion_models()
    
    # Configuraci√≥n avanzada
    parametros_avanzados()
    experimentar_temperature()
    
    # Manejo de errores y optimizaci√≥n
    manejo_errores()
    optimizacion_costos()
    
    # Informaci√≥n adicional
    diferentes_proveedores()
    mejores_practicas()
    ejercicios_avanzados()
    
    print("\nüéâ ¬°M√≥dulo 2 completado! Ahora conoces los fundamentos de LLMs.")
    print("üöÄ Pr√≥ximo m√≥dulo: Prompts y Templates Avanzados")

if __name__ == "__main__":
    main()
