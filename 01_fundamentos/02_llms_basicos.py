"""
ğŸ¤– MÃ“DULO 2: LLMs BÃSICOS
=========================

En este mÃ³dulo profundizarÃ¡s en el uso de Large Language Models con LangChain.
AprenderÃ¡s sobre diferentes modelos, configuraciones avanzadas y mejores prÃ¡cticas.

ğŸ¯ OBJETIVOS DE APRENDIZAJE:
- Entender los diferentes tipos de LLMs disponibles
- Configurar parÃ¡metros avanzados de LLMs
- Manejar errores y excepciones
- Optimizar el rendimiento y costos
- Trabajar con diferentes proveedores de LLMs

ğŸ“š CONCEPTOS CLAVE:
- Chat Models vs Completion Models
- ParÃ¡metros de configuraciÃ³n (temperature, max_tokens, etc.)
- Manejo de errores y rate limiting
- Costos y optimizaciÃ³n
- Modelos de diferentes proveedores

Autor: Tu Instructor de LangChain
Fecha: 2024
"""

import sys
import os
import time
from typing import List, Dict, Any

# Agregar el directorio raÃ­z al path para importar utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.config import config, print_config_status
from utils.helpers import print_separator, print_step, print_result, measure_execution_time

# Importaciones de LangChain
from langchain_openai import ChatOpenAI, OpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.callbacks import get_openai_callback
import openai

def tipos_llms():
    """
    ğŸ” Diferentes tipos de LLMs en LangChain
    """
    print_separator("TIPOS DE LLMs")
    
    print("""
    ğŸ¤– LangChain soporta diferentes tipos de LLMs:
    
    1. ğŸ“ Chat Models (Modelos de Chat):
       - DiseÃ±ados para conversaciones
       - Aceptan mensajes estructurados
       - Ejemplos: GPT-3.5-turbo, GPT-4, Claude
       - Mejor para aplicaciones conversacionales
    
    2. ğŸ”„ Completion Models (Modelos de CompletaciÃ³n):
       - DiseÃ±ados para completar texto
       - Aceptan texto simple
       - Ejemplos: text-davinci-003, text-curie-001
       - Mejor para generaciÃ³n de texto
    
    3. ğŸŒ Diferentes Proveedores:
       - OpenAI (GPT-3.5, GPT-4)
       - Anthropic (Claude)
       - Google (PaLM, Gemini)
       - Hugging Face (modelos locales)
       - Cohere
    """)

def chat_models_basicos():
    """
    ğŸ’¬ Trabajando con Chat Models
    """
    print_step(1, "Chat Models BÃ¡sicos", "Creando y configurando modelos de chat")
    
    try:
        # Modelo bÃ¡sico
        chat_model = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=150
        )
        
        print_result("Chat Model BÃ¡sico", chat_model)
        
        # Mensajes estructurados
        mensajes = [
            SystemMessage(content="Eres un asistente experto en programaciÃ³n Python."),
            HumanMessage(content="Â¿QuÃ© es una funciÃ³n lambda?")
        ]
        
        print(f"\nğŸ“¤ Enviando mensajes estructurados...")
        respuesta = chat_model.invoke(mensajes)
        
        print(f"ğŸ“¥ Respuesta: {respuesta.content}")
        
        return chat_model
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def completion_models():
    """
    ğŸ”„ Trabajando con Completion Models
    """
    print_step(2, "Completion Models", "Usando modelos de completaciÃ³n")
    
    try:
        # Modelo de completaciÃ³n
        completion_model = OpenAI(
            model="text-davinci-003",
            temperature=0.5,
            max_tokens=100
        )
        
        print_result("Completion Model", completion_model)
        
        # Texto simple
        prompt = "Explica quÃ© es una API REST en una frase:"
        
        print(f"\nğŸ“¤ Enviando prompt: {prompt}")
        respuesta = completion_model.invoke(prompt)
        
        print(f"ğŸ“¥ Respuesta: {respuesta}")
        
        return completion_model
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def parametros_avanzados():
    """
    âš™ï¸ ConfiguraciÃ³n avanzada de parÃ¡metros
    """
    print_step(3, "ParÃ¡metros Avanzados", "Explorando opciones de configuraciÃ³n")
    
    parametros = {
        "temperature": {
            "descripcion": "Controla la aleatoriedad (0-1)",
            "valores": [0.1, 0.5, 0.9],
            "efecto": "0=determinÃ­stico, 1=muy creativo"
        },
        "max_tokens": {
            "descripcion": "MÃ¡ximo nÃºmero de tokens en respuesta",
            "valores": [50, 100, 200],
            "efecto": "Controla la longitud de la respuesta"
        },
        "top_p": {
            "descripcion": "NÃºcleo de probabilidad (0-1)",
            "valores": [0.1, 0.5, 0.9],
            "efecto": "Controla la diversidad del vocabulario"
        },
        "frequency_penalty": {
            "descripcion": "PenalizaciÃ³n por frecuencia (-2 a 2)",
            "valores": [-1, 0, 1],
            "efecto": "Reduce repeticiÃ³n de palabras"
        },
        "presence_penalty": {
            "descripcion": "PenalizaciÃ³n por presencia (-2 a 2)",
            "valores": [-1, 0, 1],
            "efecto": "Reduce repeticiÃ³n de temas"
        }
    }
    
    print("\nâš™ï¸ PARÃMETROS DE CONFIGURACIÃ“N:")
    for param, info in parametros.items():
        print(f"\nğŸ”¹ {param.upper()}:")
        print(f"   DescripciÃ³n: {info['descripcion']}")
        print(f"   Valores tÃ­picos: {info['valores']}")
        print(f"   Efecto: {info['efecto']}")

def experimentar_temperature():
    """
    ğŸŒ¡ï¸ Experimentando con diferentes temperaturas
    """
    print_step(4, "Experimento: Temperature", "Viendo cÃ³mo afecta la creatividad")
    
    try:
        prompt = "Escribe una historia corta sobre un robot que aprende a programar."
        
        temperaturas = [0.1, 0.5, 0.9]
        
        for temp in temperaturas:
            print(f"\nğŸŒ¡ï¸ Temperature: {temp}")
            print("-" * 30)
            
            llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=temp,
                max_tokens=100
            )
            
            respuesta = llm.invoke([HumanMessage(content=prompt)])
            print(f"Respuesta: {respuesta.content[:150]}...")
            
            time.sleep(1)  # Pausa para evitar rate limiting
            
    except Exception as e:
        print(f"âŒ Error en experimento: {e}")

def manejo_errores():
    """
    ğŸ›¡ï¸ Manejo de errores y excepciones
    """
    print_step(5, "Manejo de Errores", "Aprendiendo a manejar problemas comunes")
    
    errores_comunes = {
        "Rate Limiting": "Demasiadas solicitudes en poco tiempo",
        "API Key InvÃ¡lida": "Clave de API incorrecta o expirada",
        "Modelo No Disponible": "Modelo no existe o no estÃ¡ disponible",
        "Tokens Excedidos": "Respuesta mÃ¡s larga que max_tokens",
        "Timeout": "La solicitud tardÃ³ demasiado"
    }
    
    print("\nğŸ›¡ï¸ ERRORES COMUNES Y SOLUCIONES:")
    for error, descripcion in errores_comunes.items():
        print(f"   ğŸ”¹ {error}: {descripcion}")
    
    # Ejemplo de manejo de errores
    try:
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=50,
            request_timeout=10  # Timeout de 10 segundos
        )
        
        respuesta = llm.invoke([HumanMessage(content="Hola")])
        print(f"\nâœ… Solicitud exitosa: {respuesta.content}")
        
    except openai.RateLimitError:
        print("âŒ Rate limit alcanzado. Espera un momento.")
    except openai.AuthenticationError:
        print("âŒ Error de autenticaciÃ³n. Verifica tu API key.")
    except openai.APITimeoutError:
        print("âŒ Timeout. La solicitud tardÃ³ demasiado.")
    except Exception as e:
        print(f"âŒ Error inesperado: {e}")

def optimizacion_costos():
    """
    ğŸ’° OptimizaciÃ³n de costos
    """
    print_step(6, "OptimizaciÃ³n de Costos", "Maximizando eficiencia y minimizando gastos")
    
    print("""
    ğŸ’° ESTRATEGIAS DE OPTIMIZACIÃ“N:
    
    1. ğŸ“ Control de Longitud:
       - Usa max_tokens apropiado
       - Limita el contexto de entrada
       - Evita prompts innecesariamente largos
    
    2. ğŸ¯ Modelos Eficientes:
       - GPT-3.5-turbo es mÃ¡s barato que GPT-4
       - Usa modelos mÃ¡s pequeÃ±os para tareas simples
       - Considera modelos locales para desarrollo
    
    3. ğŸ”„ Caching:
       - Guarda respuestas frecuentes
       - Reutiliza resultados similares
       - Implementa cache local
    
    4. ğŸ“Š Monitoreo:
       - Usa callbacks para tracking
       - Monitorea uso de tokens
       - Establece lÃ­mites de gasto
    """)
    
    # Ejemplo con callback para monitoreo
    try:
        with get_openai_callback() as cb:
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)
            respuesta = llm.invoke([HumanMessage(content="Explica quÃ© es Python en una frase.")])
            
            print(f"\nğŸ“Š ESTADÃSTICAS DE USO:")
            print(f"   Tokens totales: {cb.total_tokens}")
            print(f"   Tokens de prompt: {cb.prompt_tokens}")
            print(f"   Tokens de respuesta: {cb.completion_tokens}")
            print(f"   Costo total: ${cb.total_cost}")
            
    except Exception as e:
        print(f"âŒ Error en monitoreo: {e}")

def diferentes_proveedores():
    """
    ğŸŒ Trabajando con diferentes proveedores
    """
    print_step(7, "Diferentes Proveedores", "Explorando opciones de LLMs")
    
    proveedores = {
        "OpenAI": {
            "modelos": ["gpt-3.5-turbo", "gpt-4", "text-davinci-003"],
            "ventajas": "Muy estable, buena documentaciÃ³n",
            "desventajas": "Puede ser costoso"
        },
        "Anthropic": {
            "modelos": ["claude-2", "claude-instant"],
            "ventajas": "Muy bueno para anÃ¡lisis",
            "desventajas": "API mÃ¡s nueva"
        },
        "Google": {
            "modelos": ["text-bison", "chat-bison"],
            "ventajas": "IntegraciÃ³n con Google Cloud",
            "desventajas": "Menos documentaciÃ³n"
        },
        "Hugging Face": {
            "modelos": ["miles de modelos locales"],
            "ventajas": "Gratis, privado",
            "desventajas": "Requiere mÃ¡s recursos"
        }
    }
    
    print("\nğŸŒ PROVEEDORES DE LLMs:")
    for proveedor, info in proveedores.items():
        print(f"\nğŸ”¹ {proveedor}:")
        print(f"   Modelos: {', '.join(info['modelos'])}")
        print(f"   Ventajas: {info['ventajas']}")
        print(f"   Desventajas: {info['desventajas']}")

def ejercicios_avanzados():
    """
    ğŸ¯ Ejercicios avanzados para prÃ¡ctica
    """
    print_step(8, "Ejercicios Avanzados", "Pon en prÃ¡ctica lo aprendido")
    
    ejercicios = [
        {
            "titulo": "Comparador de Modelos",
            "descripcion": "Crea una funciÃ³n que compare respuestas de diferentes modelos",
            "objetivo": "Entender diferencias entre modelos"
        },
        {
            "titulo": "Optimizador de Prompts",
            "descripcion": "Crea prompts que minimicen tokens pero maximicen calidad",
            "objetivo": "Aprender optimizaciÃ³n de costos"
        },
        {
            "titulo": "Manejador de Errores",
            "descripcion": "Implementa un sistema robusto de manejo de errores",
            "objetivo": "Crear aplicaciones robustas"
        },
        {
            "titulo": "Monitor de Costos",
            "descripcion": "Crea un sistema que monitoree y limite gastos",
            "objetivo": "Control de presupuesto"
        }
    ]
    
    print("\nğŸ¯ EJERCICIOS AVANZADOS:")
    for i, ejercicio in enumerate(ejercicios, 1):
        print(f"\n{i}. {ejercicio['titulo']}")
        print(f"   DescripciÃ³n: {ejercicio['descripcion']}")
        print(f"   Objetivo: {ejercicio['objetivo']}")

def mejores_practicas():
    """
    â­ Mejores prÃ¡cticas para LLMs
    """
    print_step(9, "Mejores PrÃ¡cticas", "Consejos para usar LLMs efectivamente")
    
    practicas = [
        "ğŸ¯ Siempre especifica el contexto y rol del modelo",
        "ğŸ“ Usa max_tokens apropiado para tu caso de uso",
        "ğŸŒ¡ï¸ Ajusta temperature segÃºn la tarea (baja para hechos, alta para creatividad)",
        "ğŸ›¡ï¸ Implementa manejo de errores robusto",
        "ğŸ’° Monitorea costos y optimiza cuando sea posible",
        "ğŸ”„ Usa caching para respuestas frecuentes",
        "ğŸ“Š Implementa logging para debugging",
        "âš¡ Considera modelos mÃ¡s pequeÃ±os para tareas simples",
        "ğŸ”’ Maneja API keys de forma segura",
        "ğŸ“š Documenta tus prompts y configuraciones"
    ]
    
    print("\nâ­ MEJORES PRÃCTICAS:")
    for practica in practicas:
        print(f"   {practica}")

def main():
    """
    ğŸ¯ FunciÃ³n principal del mÃ³dulo
    """
    print_separator("MÃ“DULO 2: LLMs BÃSICOS")
    
    # Verificar configuraciÃ³n
    if not config.validate_config():
        return
    
    # Contenido del mÃ³dulo
    tipos_llms()
    
    # Ejemplos prÃ¡cticos
    chat_models_basicos()
    completion_models()
    
    # ConfiguraciÃ³n avanzada
    parametros_avanzados()
    experimentar_temperature()
    
    # Manejo de errores y optimizaciÃ³n
    manejo_errores()
    optimizacion_costos()
    
    # InformaciÃ³n adicional
    diferentes_proveedores()
    mejores_practicas()
    ejercicios_avanzados()
    
    print("\nğŸ‰ Â¡MÃ³dulo 2 completado! Ahora conoces los fundamentos de LLMs.")
    print("ğŸš€ PrÃ³ximo mÃ³dulo: Prompts y Templates Avanzados")

if __name__ == "__main__":
    main()
