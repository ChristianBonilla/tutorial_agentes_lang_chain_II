#!/usr/bin/env python3
"""
============================================================
    M√ìDULO 9: RAG AVANZADO CON T√âCNICAS MODERNAS
============================================================

üéØ OBJETIVOS:
- Implementar b√∫squeda h√≠brida (dense + sparse)
- Query expansion y reformulaci√≥n
- Reranking de resultados
- RAG con m√∫ltiples fuentes
- Optimizaci√≥n de prompts para RAG

üìö CONTENIDO:
1. B√∫squeda h√≠brida
2. Query expansion
3. Reranking avanzado
4. RAG multi-fuente
5. Prompts optimizados
6. Evaluaci√≥n de RAG
"""

import asyncio
import time
import json
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import os
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv()

# Importaciones de LangChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Pinecone as LangChainPinecone
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers.multi_query import MultiQueryRetriever

# Importaciones locales
import sys
sys.path.append('..')
from utils.config import LangChainConfig
from utils.pinecone_config import PineconeConfig
from utils.helpers import print_separator, print_step, print_result

# Configuraci√≥n
config = LangChainConfig()
pinecone_config = PineconeConfig()

@dataclass
class RAGResult:
    """Resultado de RAG"""
    query: str
    answer: str
    sources: List[Document]
    confidence: float
    execution_time: float

class AdvancedRAG:
    """Sistema RAG avanzado con m√∫ltiples t√©cnicas"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model=config.default_model, temperature=0.3)
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = None
        self.setup_vectorstore()
    
    def setup_vectorstore(self):
        """Configurar vector store"""
        try:
            from pinecone import Pinecone
            pc = Pinecone(api_key=pinecone_config.api_key)
            
            self.vectorstore = LangChainPinecone.from_existing_index(
                index_name=pinecone_config.index_name,
                embedding=self.embeddings
            )
            print("‚úÖ Vector store configurado")
        except Exception as e:
            print(f"‚ùå Error configurando vector store: {e}")
    
    def hybrid_search(self, query: str, k: int = 5) -> List[Document]:
        """B√∫squeda h√≠brida combinando dense y sparse retrieval"""
        print_step(1, "B√∫squeda H√≠brida", "Implementando b√∫squeda combinada")
        
        # B√∫squeda por similitud (dense)
        dense_results = self.vectorstore.similarity_search(query, k=k)
        
        # B√∫squeda por similitud con score (dense con scores)
        dense_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)
        
        # B√∫squeda por similitud m√°xima (sparse-like)
        max_marginal_results = self.vectorstore.max_marginal_relevance_search(query, k=k)
        
        # Combinar resultados
        all_results = []
        seen_ids = set()
        
        # Agregar resultados densos
        for doc in dense_results:
            if doc.page_content not in seen_ids:
                all_results.append(doc)
                seen_ids.add(doc.page_content)
        
        # Agregar resultados MMR
        for doc in max_marginal_results:
            if doc.page_content not in seen_ids:
                all_results.append(doc)
                seen_ids.add(doc.page_content)
        
        print(f"üîç B√∫squeda h√≠brida completada: {len(all_results)} documentos")
        return all_results[:k]
    
    def query_expansion(self, query: str) -> List[str]:
        """Expansi√≥n de consultas para mejorar recuperaci√≥n"""
        print_step(2, "Query Expansion", "Generando variaciones de consulta")
        
        expansion_prompt = PromptTemplate(
            input_variables=["query"],
            template="""
            Genera 3 variaciones de la siguiente consulta para mejorar la b√∫squeda.
            Las variaciones deben ser sem√°nticamente similares pero usar diferentes palabras.
            
            Consulta original: {query}
            
            Variaciones:
            1. 
            2. 
            3. 
            """
        )
        
        expansion_chain = expansion_prompt.format(query=query)
        response = self.llm.invoke([HumanMessage(content=expansion_chain)])
        
        # Parsear variaciones
        lines = response.content.strip().split('\n')
        variations = []
        for line in lines:
            if line.strip() and not line.startswith('Variaciones:'):
                variation = line.strip().split('. ', 1)[-1] if '. ' in line else line.strip()
                variations.append(variation)
        
        print(f"üîÑ Variaciones generadas: {variations}")
        return variations
    
    def multi_query_retrieval(self, query: str, k: int = 5) -> List[Document]:
        """Recuperaci√≥n con m√∫ltiples consultas"""
        print_step(3, "Multi-Query Retrieval", "Usando m√∫ltiples consultas")
        
        # Crear retriever multi-query
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=self.vectorstore.as_retriever(),
            llm=self.llm
        )
        
        # Ejecutar b√∫squeda
        results = multi_query_retriever.get_relevant_documents(query)
        
        print(f"üîç Multi-query retrieval: {len(results)} documentos")
        return results[:k]
    
    def contextual_compression(self, query: str, k: int = 5) -> List[Document]:
        """Compresi√≥n contextual de documentos"""
        print_step(4, "Compresi√≥n Contextual", "Comprimiendo documentos relevantes")
        
        # Crear compressor
        compressor_prompt = PromptTemplate(
            input_variables=["question", "context"],
            template="""
            Bas√°ndote en la pregunta, extrae solo la informaci√≥n relevante del contexto.
            
            Pregunta: {question}
            Contexto: {context}
            
            Informaci√≥n relevante:
            """
        )
        
        compressor = LLMChainExtractor.from_llm(self.llm, prompt=compressor_prompt)
        
        # Crear retriever con compresi√≥n
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=self.vectorstore.as_retriever()
        )
        
        # Ejecutar b√∫squeda
        results = compression_retriever.get_relevant_documents(query)
        
        print(f"üóúÔ∏è Compresi√≥n contextual: {len(results)} documentos")
        return results[:k]
    
    def advanced_rag_chain(self, query: str) -> RAGResult:
        """Chain RAG avanzada con m√∫ltiples t√©cnicas"""
        print_step(5, "RAG Avanzado", "Implementando RAG con t√©cnicas avanzadas")
        
        start_time = time.time()
        
        # 1. Query expansion
        query_variations = self.query_expansion(query)
        
        # 2. B√∫squeda h√≠brida con query original
        hybrid_results = self.hybrid_search(query, k=3)
        
        # 3. Multi-query retrieval
        multi_query_results = self.multi_query_retrieval(query, k=3)
        
        # 4. Compresi√≥n contextual
        compressed_results = self.contextual_compression(query, k=3)
        
        # Combinar todos los resultados
        all_docs = hybrid_results + multi_query_results + compressed_results
        
        # Eliminar duplicados
        unique_docs = []
        seen_content = set()
        for doc in all_docs:
            if doc.page_content not in seen_content:
                unique_docs.append(doc)
                seen_content.add(doc.page_content)
        
        # Prompt optimizado para RAG
        rag_prompt = PromptTemplate(
            input_variables=["context", "question"],
            template="""
            Eres un asistente experto. Responde la pregunta bas√°ndote √∫nicamente en el contexto proporcionado.
            
            Si la informaci√≥n no est√° en el contexto, di "No tengo suficiente informaci√≥n para responder".
            
            Contexto:
            {context}
            
            Pregunta: {question}
            
            Respuesta:
            """
        )
        
        # Crear contexto
        context = "\n\n".join([doc.page_content for doc in unique_docs[:5]])
        
        # Generar respuesta
        prompt = rag_prompt.format(context=context, question=query)
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        execution_time = time.time() - start_time
        
        # Calcular confianza (simplificado)
        confidence = min(0.9, len(unique_docs) / 10)
        
        return RAGResult(
            query=query,
            answer=response.content,
            sources=unique_docs[:3],
            confidence=confidence,
            execution_time=execution_time
        )
    
    def evaluate_rag(self, test_queries: List[str]) -> Dict[str, Any]:
        """Evaluaci√≥n del sistema RAG"""
        print_step(6, "Evaluaci√≥n RAG", "Evaluando rendimiento del sistema")
        
        results = []
        
        for query in test_queries:
            try:
                result = self.advanced_rag_chain(query)
                results.append(result)
                print(f"‚úÖ Query: {query[:50]}... | Tiempo: {result.execution_time:.2f}s")
            except Exception as e:
                print(f"‚ùå Error en query '{query}': {e}")
        
        # M√©tricas de evaluaci√≥n
        avg_time = sum(r.execution_time for r in results) / len(results)
        avg_confidence = sum(r.confidence for r in results) / len(results)
        avg_sources = sum(len(r.sources) for r in results) / len(results)
        
        evaluation = {
            'total_queries': len(test_queries),
            'successful_queries': len(results),
            'avg_execution_time': avg_time,
            'avg_confidence': avg_confidence,
            'avg_sources_per_query': avg_sources,
            'success_rate': len(results) / len(test_queries) * 100
        }
        
        print_result("Evaluaci√≥n RAG", evaluation)
        return evaluation

def main():
    """Funci√≥n principal"""
    print_separator()
    print("             M√ìDULO 9: RAG AVANZADO CON T√âCNICAS MODERNAS             ")
    print_separator()
    
    # Validar configuraci√≥n
    if not config.validate_config():
        print("‚ùå Error en configuraci√≥n")
        return
    
    print("‚úÖ Configuraci√≥n validada correctamente")
    
    # Crear sistema RAG avanzado
    advanced_rag = AdvancedRAG()
    
    # Queries de prueba
    test_queries = [
        "¬øQu√© es LangChain y para qu√© se usa?",
        "¬øCu√°les son las principales caracter√≠sticas de LangChain?",
        "¬øQu√© es RAG y c√≥mo funciona?",
        "¬øCu√°les son los casos de uso m√°s comunes de LangChain?"
    ]
    
    # Probar RAG avanzado
    for query in test_queries:
        print(f"\nüîç Procesando: {query}")
        result = advanced_rag.advanced_rag_chain(query)
        print(f"ü§ñ Respuesta: {result.answer[:100]}...")
        print(f"üìö Fuentes: {len(result.sources)} documentos")
        print(f"‚è±Ô∏è Tiempo: {result.execution_time:.2f}s")
        print(f"üéØ Confianza: {result.confidence:.2f}")
    
    # Evaluar sistema
    advanced_rag.evaluate_rag(test_queries)
    
    print_separator()
    print("üéâ ¬°M√≥dulo 9 completado! Ahora dominas RAG avanzado.")
    print("üöÄ Pr√≥ximo m√≥dulo: Tools Personalizados")

if __name__ == "__main__":
    main()

