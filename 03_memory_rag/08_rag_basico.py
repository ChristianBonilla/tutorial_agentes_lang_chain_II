"""
ğŸ” MÃ“DULO 8: RAG BÃSICO CON PINECONE
====================================

En este mÃ³dulo aprenderÃ¡s a implementar RAG (Retrieval Augmented Generation)
usando Pinecone como base de datos vectorial para almacenar y recuperar embeddings.

ğŸ¯ OBJETIVOS DE APRENDIZAJE:
- Entender el concepto de RAG
- Configurar Pinecone para almacenar embeddings
- Crear un sistema de recuperaciÃ³n de informaciÃ³n
- Implementar RAG bÃ¡sico con LangChain
- Optimizar bÃºsquedas vectoriales

ğŸ“š CONCEPTOS CLAVE:
- RAG (Retrieval Augmented Generation)
- Embeddings y Vector Search
- Pinecone Integration
- Document Loading
- Text Splitting
- Similarity Search

Autor: Tu Instructor de LangChain
Fecha: 2024
"""

import sys
import os
from typing import List, Dict, Any

# Agregar el directorio raÃ­z al path para importar utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.config import config
from utils.helpers import print_separator, print_step, print_result
from utils.pinecone_config import pinecone_config, print_pinecone_status

# Importaciones de LangChain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Pinecone as LangChainPinecone
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
from langchain.schema import Document

def introduccion_rag():
    """
    ğŸ“ IntroducciÃ³n al RAG
    """
    print_separator("Â¿QUÃ‰ ES RAG?")
    
    print("""
    ğŸ” RAG (Retrieval Augmented Generation) es una tÃ©cnica que combina:
    
    1. ğŸ“š Retrieval (RecuperaciÃ³n): Buscar informaciÃ³n relevante en una base de datos
    2. ğŸ¤– Generation (GeneraciÃ³n): Usar esa informaciÃ³n para generar respuestas
    
    ğŸ—ï¸ ARQUITECTURA RAG:
    
    [Query] â†’ [Embedding] â†’ [Vector Search] â†’ [Retrieved Docs] â†’ [LLM] â†’ [Response]
    
    ğŸ“‹ COMPONENTES PRINCIPALES:
    
    1. ğŸ”¤ Text Splitter: Divide documentos en chunks
    2. ğŸ§® Embeddings: Convierte texto en vectores
    3. ğŸ—„ï¸ Vector Store: Almacena y busca vectores
    4. ğŸ” Retriever: Recupera documentos relevantes
    5. ğŸ¤– LLM: Genera respuestas basadas en contexto
    
    ğŸ’¡ VENTAJAS:
    - Respuestas mÃ¡s precisas y actualizadas
    - Acceso a informaciÃ³n especÃ­fica
    - ReducciÃ³n de alucinaciones
    - Escalabilidad con grandes volÃºmenes de datos
    """)

def configuracion_pinecone():
    """
    ğŸŒ² ConfiguraciÃ³n de Pinecone para RAG
    """
    print_step(1, "ConfiguraciÃ³n de Pinecone", "Preparando la base de datos vectorial")
    
    # Mostrar estado de Pinecone
    print_pinecone_status()
    
    # Validar configuraciÃ³n
    if not pinecone_config.validate_config():
        print("âŒ ConfiguraciÃ³n de Pinecone incompleta")
        print("   AsegÃºrate de configurar PINECONE_API_KEY y PINECONE_ENVIRONMENT en .env")
        return None
    
    # Obtener o crear Ã­ndice
    index = pinecone_config.get_or_create_index()
    if not index:
        print("âŒ No se pudo obtener el Ã­ndice de Pinecone")
        return None
    
    print("âœ… Pinecone configurado correctamente para RAG")
    return index

def crear_embeddings():
    """
    ğŸ§® Creando embeddings con OpenAI
    """
    print_step(2, "Creando Embeddings", "Configurando el modelo de embeddings")
    
    try:
        # Crear modelo de embeddings
        embeddings = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=config.openai_api_key
        )
        
        print_result("Embeddings Model", embeddings)
        
        # Probar embeddings
        test_texts = [
            "LangChain es una biblioteca para LLMs",
            "Pinecone es una base de datos vectorial",
            "RAG combina recuperaciÃ³n y generaciÃ³n"
        ]
        
        print("\nğŸ§ª Probando embeddings:")
        for text in test_texts:
            embedding = embeddings.embed_query(text)
            print(f"ğŸ“ '{text[:30]}...' â†’ Vector de {len(embedding)} dimensiones")
        
        return embeddings
        
    except Exception as e:
        print(f"âŒ Error al crear embeddings: {e}")
        return None

def cargar_y_dividir_documentos():
    """
    ğŸ“„ Cargando y dividiendo documentos
    """
    print_step(3, "Cargar y Dividir Documentos", "Preparando documentos para RAG")
    
    try:
        # Cargar documento de ejemplo
        loader = TextLoader("data/sample_documents/sample_text.txt")
        documents = loader.load()
        
        print(f"âœ… Documento cargado: {len(documents)} documento(s)")
        print(f"ğŸ“ Longitud del texto: {len(documents[0].page_content)} caracteres")
        
        # Dividir documentos en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
        chunks = text_splitter.split_documents(documents)
        
        print(f"âœ… Documentos divididos en {len(chunks)} chunks")
        print(f"ğŸ“ TamaÃ±o promedio de chunk: {len(chunks[0].page_content)} caracteres")
        
        # Mostrar algunos chunks
        print("\nğŸ“„ Ejemplos de chunks:")
        for i, chunk in enumerate(chunks[:3]):
            print(f"\nChunk {i+1}:")
            print(f"   {chunk.page_content[:100]}...")
        
        return chunks
        
    except Exception as e:
        print(f"âŒ Error al cargar documentos: {e}")
        return None

def crear_vectorstore():
    """
    ğŸ—„ï¸ Creando vector store con Pinecone
    """
    print_step(4, "Crear Vector Store", "Almacenando embeddings en Pinecone")
    
    try:
        # Obtener Ã­ndice de Pinecone
        index = pinecone_config.get_or_create_index()
        if not index:
            return None
        
        # Crear embeddings
        embeddings = OpenAIEmbeddings()
        
        # Cargar y dividir documentos
        chunks = cargar_y_dividir_documentos()
        if not chunks:
            return None
        
        # Crear vector store usando la nueva API de Pinecone
        from pinecone import Pinecone
        pc = Pinecone(api_key=pinecone_config.api_key)
        
        # Crear vector store
        vectorstore = LangChainPinecone.from_documents(
            documents=chunks,
            embedding=embeddings,
            index_name=pinecone_config.index_name
        )
        
        print_result("Vector Store Creado", vectorstore)
        print(f"âœ… {len(chunks)} documentos almacenados en Pinecone")
        
        return vectorstore
        
    except Exception as e:
        print(f"âŒ Error al crear vector store: {e}")
        return None

def bÃºsqueda_similitud():
    """
    ğŸ” BÃºsqueda por similitud
    """
    print_step(5, "BÃºsqueda por Similitud", "Probando recuperaciÃ³n de documentos")
    
    try:
        # Obtener vector store
        embeddings = OpenAIEmbeddings()
        from pinecone import Pinecone
        pc = Pinecone(api_key=pinecone_config.api_key)
        
        vectorstore = LangChainPinecone.from_existing_index(
            index_name=pinecone_config.index_name,
            embedding=embeddings
        )
        
        # Consultas de prueba
        queries = [
            "Â¿QuÃ© es LangChain?",
            "Â¿CuÃ¡les son las caracterÃ­sticas principales?",
            "Â¿QuÃ© es RAG?",
            "Â¿CÃ³mo funciona la memoria en LangChain?"
        ]
        
        print("\nğŸ” Probando bÃºsquedas:")
        for query in queries:
            print(f"\nğŸ“ Consulta: {query}")
            
            # Buscar documentos similares
            docs = vectorstore.similarity_search(query, k=2)
            
            for i, doc in enumerate(docs):
                print(f"   Resultado {i+1}: {doc.page_content[:100]}...")
        
        return vectorstore
        
    except Exception as e:
        print(f"âŒ Error en bÃºsqueda: {e}")
        return None

def rag_basico():
    """
    ğŸ¤– RAG bÃ¡sico con RetrievalQA
    """
    print_step(6, "RAG BÃ¡sico", "Implementando RAG completo")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Obtener vector store
        embeddings = OpenAIEmbeddings()
        from pinecone import Pinecone
        pc = Pinecone(api_key=pinecone_config.api_key)
        
        vectorstore = LangChainPinecone.from_existing_index(
            index_name=pinecone_config.index_name,
            embedding=embeddings
        )
        
        # Crear retriever
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 3}
        )
        
        # Crear chain de RAG
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            verbose=True
        )
        
        print_result("RAG Chain", qa_chain)
        
        # Probar RAG
        questions = [
            "Â¿QuÃ© es LangChain y para quÃ© se usa?",
            "Â¿CuÃ¡les son las principales caracterÃ­sticas de LangChain?",
            "Â¿QuÃ© es RAG y cÃ³mo funciona?",
            "Â¿CuÃ¡les son los casos de uso mÃ¡s comunes?"
        ]
        
        print("\nğŸ¤– Probando RAG:")
        for question in questions:
            print(f"\nâ“ Pregunta: {question}")
            
            try:
                result = qa_chain({"query": question})
                print(f"ğŸ¤– Respuesta: {result['result']}")
                
                # Mostrar fuentes
                print("ğŸ“š Fuentes:")
                for i, doc in enumerate(result['source_documents'][:2]):
                    print(f"   {i+1}. {doc.page_content[:100]}...")
                    
            except Exception as e:
                print(f"âŒ Error al procesar pregunta: {e}")
        
        return qa_chain
        
    except Exception as e:
        print(f"âŒ Error en RAG: {e}")
        return None

def optimizacion_rag():
    """
    âš¡ OptimizaciÃ³n de RAG
    """
    print_step(7, "OptimizaciÃ³n de RAG", "Mejorando el rendimiento")
    
    print("""
    âš¡ ESTRATEGIAS DE OPTIMIZACIÃ“N:
    
    1. ğŸ“ Chunk Size: Ajustar tamaÃ±o de chunks segÃºn el contenido
    2. ğŸ”„ Chunk Overlap: Configurar solapamiento para contexto
    3. ğŸ¯ Top-K: Seleccionar nÃºmero Ã³ptimo de documentos
    4. ğŸ” Search Type: Elegir entre similarity, mmr, etc.
    5. ğŸ“Š Metadata: Usar metadatos para filtrado
    6. ğŸ§® Embedding Model: Seleccionar modelo apropiado
    """)
    
    try:
        # ConfiguraciÃ³n optimizada
        embeddings = OpenAIEmbeddings()
        from pinecone import Pinecone
        pc = Pinecone(api_key=pinecone_config.api_key)
        
        vectorstore = LangChainPinecone.from_existing_index(
            index_name=pinecone_config.index_name,
            embedding=embeddings
        )
        
        # Retriever optimizado
        retriever = vectorstore.as_retriever(
            search_type="mmr",  # Maximum Marginal Relevance
            search_kwargs={
                "k": 5,
                "fetch_k": 10,
                "lambda_mult": 0.7
            }
        )
        
        # LLM optimizado
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.3,  # Menos creativo, mÃ¡s preciso
            max_tokens=500
        )
        
        # Chain optimizada
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="map_reduce",  # Para documentos largos
            retriever=retriever,
            return_source_documents=True,
            verbose=False
        )
        
        print_result("RAG Optimizado", qa_chain)
        
        # Probar optimizaciÃ³n
        question = "Explica detalladamente quÃ© es LangChain y sus principales caracterÃ­sticas"
        print(f"\nâ“ Pregunta compleja: {question}")
        
        result = qa_chain({"query": question})
        print(f"ğŸ¤– Respuesta optimizada: {result['result']}")
        
        return qa_chain
        
    except Exception as e:
        print(f"âŒ Error en optimizaciÃ³n: {e}")
        return None

def ejercicios_practicos():
    """
    ğŸ¯ Ejercicios prÃ¡cticos
    """
    print_step(8, "Ejercicios PrÃ¡cticos", "Pon en prÃ¡ctica lo aprendido")
    
    ejercicios = [
        {
            "titulo": "RAG con Documentos Personalizados",
            "descripcion": "Crea un RAG con tus propios documentos",
            "objetivo": "Practicar carga y procesamiento de documentos"
        },
        {
            "titulo": "OptimizaciÃ³n de Chunks",
            "descripcion": "Experimenta con diferentes tamaÃ±os de chunks",
            "objetivo": "Aprender optimizaciÃ³n de text splitting"
        },
        {
            "titulo": "BÃºsqueda Avanzada",
            "descripcion": "Implementa diferentes tipos de bÃºsqueda",
            "objetivo": "Practicar configuraciÃ³n de retrievers"
        },
        {
            "titulo": "RAG con Metadatos",
            "descripcion": "Usa metadatos para filtrado y organizaciÃ³n",
            "objetivo": "Aprender gestiÃ³n de metadatos"
        },
        {
            "titulo": "EvaluaciÃ³n de RAG",
            "descripcion": "Crea mÃ©tricas para evaluar calidad de respuestas",
            "objetivo": "Aprender evaluaciÃ³n de sistemas RAG"
        }
    ]
    
    print("\nğŸ¯ EJERCICIOS PRÃCTICOS:")
    for i, ejercicio in enumerate(ejercicios, 1):
        print(f"\n{i}. {ejercicio['titulo']}")
        print(f"   DescripciÃ³n: {ejercicio['descripcion']}")
        print(f"   Objetivo: {ejercicio['objetivo']}")

def mejores_practicas():
    """
    â­ Mejores prÃ¡cticas para RAG
    """
    print_step(9, "Mejores PrÃ¡cticas", "Consejos para implementar RAG efectivamente")
    
    practicas = [
        "ğŸ“ Ajusta el tamaÃ±o de chunks segÃºn el tipo de contenido",
        "ğŸ”„ Usa overlap apropiado para mantener contexto",
        "ğŸ¯ Selecciona el nÃºmero correcto de documentos (k)",
        "ğŸ” Experimenta con diferentes tipos de bÃºsqueda",
        "ğŸ“Š Usa metadatos para organizar y filtrar documentos",
        "ğŸ§® Elige el modelo de embeddings apropiado",
        "âš¡ Optimiza la configuraciÃ³n del LLM para RAG",
        "ğŸ“ Documenta la estructura de tus documentos",
        "ğŸ§ª Prueba con diferentes consultas y casos de uso",
        "ğŸ“ˆ Monitorea la calidad de las respuestas"
    ]
    
    print("\nâ­ MEJORES PRÃCTICAS:")
    for practica in practicas:
        print(f"   {practica}")

def main():
    """
    ğŸ¯ FunciÃ³n principal del mÃ³dulo
    """
    print_separator("MÃ“DULO 8: RAG BÃSICO CON PINECONE")
    
    # Verificar configuraciÃ³n
    if not config.validate_config():
        return
    
    # Verificar Pinecone
    if not pinecone_config.validate_config():
        print("âš ï¸ Pinecone no estÃ¡ configurado. Algunas funciones no estarÃ¡n disponibles.")
    
    # Contenido del mÃ³dulo
    introduccion_rag()
    
    # ConfiguraciÃ³n
    configuracion_pinecone()
    embeddings = crear_embeddings()
    
    if embeddings:
        # Procesamiento de documentos
        cargar_y_dividir_documentos()
        vectorstore = crear_vectorstore()
        
        if vectorstore:
            # RAG
            bÃºsqueda_similitud()
            rag_basico()
            optimizacion_rag()
    
    # ConsolidaciÃ³n
    mejores_practicas()
    ejercicios_practicos()
    
    print("\nğŸ‰ Â¡MÃ³dulo 8 completado! Ahora dominas RAG bÃ¡sico con Pinecone.")
    print("ğŸš€ PrÃ³ximo mÃ³dulo: RAG Avanzado")

if __name__ == "__main__":
    main()
