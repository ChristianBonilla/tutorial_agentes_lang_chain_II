"""
üîó M√ìDULO 4: CHAINS SIMPLES
===========================

En este m√≥dulo aprender√°s a crear y usar chains (cadenas) en LangChain.
Las chains son secuencias de operaciones que combinan diferentes componentes
para crear flujos de procesamiento m√°s complejos.

üéØ OBJETIVOS DE APRENDIZAJE:
- Entender qu√© son las chains y c√≥mo funcionan
- Crear chains simples y secuenciales
- Combinar diferentes tipos de chains
- Optimizar el rendimiento de las chains
- Manejar errores en chains

üìö CONCEPTOS CLAVE:
- LLMChain
- SequentialChain
- RouterChain
- Chain Composition
- Error Handling
- Performance Optimization

Autor: Tu Instructor de LangChain
Fecha: 2024
"""

import sys
import os
from typing import List, Dict, Any

# Agregar el directorio ra√≠z al path para importar utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.config import config
from utils.helpers import print_separator, print_step, print_result

# Importaciones de LangChain
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain
from langchain.schema import BaseOutputParser
from langchain.callbacks import get_openai_callback

def introduccion_chains():
    """
    üéì Introducci√≥n a las Chains
    """
    print_separator("¬øQU√â SON LAS CHAINS?")
    
    print("""
    üîó Las Chains son el coraz√≥n de LangChain. Son secuencias de operaciones
    que combinan diferentes componentes para crear flujos de procesamiento.
    
    üèóÔ∏è ARQUITECTURA DE CHAINS:
    
    Input ‚Üí [Chain 1] ‚Üí [Chain 2] ‚Üí [Chain 3] ‚Üí Output
    
    üìã TIPOS DE CHAINS:
    
    1. üîÑ LLMChain: La m√°s b√°sica, combina LLM + Prompt
    2. üîó SequentialChain: Ejecuta chains en secuencia
    3. üõ£Ô∏è RouterChain: Dirige el flujo seg√∫n condiciones
    4. üîÄ TransformChain: Transforma datos entre chains
    5. üéØ CustomChain: Chains personalizadas
    
    üí° VENTAJAS:
    - Modularidad y reutilizaci√≥n
    - Flujos complejos y organizados
    - F√°cil debugging y testing
    - Composici√≥n flexible
    """)

def llm_chain_basica():
    """
    üîÑ LLMChain b√°sica
    """
    print_step(1, "LLMChain B√°sica", "Creando tu primera chain")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7,
            max_tokens=150
        )
        
        # Crear prompt template
        prompt = PromptTemplate(
            input_variables=["concepto"],
            template="Explica qu√© es {concepto} en el contexto de programaci√≥n. M√°ximo 2 frases."
        )
        
        # Crear LLMChain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("LLMChain Creada", chain)
        
        # Usar la chain
        conceptos = ["API", "JSON", "REST"]
        
        print("\nüîç Probando la chain:")
        for concepto in conceptos:
            resultado = chain.run(concepto)
            print(f"üìù {concepto}: {resultado}")
        
        return chain
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def llm_chain_con_variables():
    """
    üîß LLMChain con m√∫ltiples variables
    """
    print_step(2, "LLMChain con Variables", "Chains m√°s complejas")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Prompt con m√∫ltiples variables
        prompt = PromptTemplate(
            input_variables=["lenguaje", "concepto", "nivel"],
            template="""
            Eres un instructor de programaci√≥n experto en {lenguaje}.
            Explica el concepto de {concepto} a nivel {nivel}.
            Mant√©n la explicaci√≥n clara y concisa.
            """
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("Chain con Variables", chain)
        
        # Ejemplos de uso
        ejemplos = [
            {"lenguaje": "Python", "concepto": "decoradores", "nivel": "intermedio"},
            {"lenguaje": "JavaScript", "concepto": "closures", "nivel": "avanzado"},
            {"lenguaje": "Java", "concepto": "polimorfismo", "nivel": "b√°sico"}
        ]
        
        print("\nüîç Probando con diferentes variables:")
        for ejemplo in ejemplos:
            resultado = chain.run(ejemplo)
            print(f"\nüìù {ejemplo['lenguaje']} - {ejemplo['concepto']} ({ejemplo['nivel']}):")
            print(f"   {resultado}")
        
        return chain
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def simple_sequential_chain():
    """
    üîó SimpleSequentialChain - Chains en secuencia
    """
    print_step(3, "SimpleSequentialChain", "Ejecutando chains en secuencia")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Chain 1: Generar concepto
        prompt1 = PromptTemplate(
            input_variables=["tema"],
            template="Genera un concepto de programaci√≥n relacionado con {tema}. Solo el nombre del concepto."
        )
        chain1 = LLMChain(llm=llm, prompt=prompt1)
        
        # Chain 2: Explicar concepto
        prompt2 = PromptTemplate(
            input_variables=["concepto"],
            template="Explica qu√© es {concepto} en programaci√≥n. M√°ximo 2 frases."
        )
        chain2 = LLMChain(llm=llm, prompt=prompt2)
        
        # Crear sequential chain
        sequential_chain = SimpleSequentialChain(
            chains=[chain1, chain2],
            verbose=True
        )
        
        print_result("Sequential Chain", sequential_chain)
        
        # Probar la chain
        temas = ["estructuras de datos", "algoritmos", "patrones de dise√±o"]
        
        print("\nüîó Probando sequential chain:")
        for tema in temas:
            resultado = sequential_chain.run(tema)
            print(f"\nüìù Tema: {tema}")
            print(f"   Resultado: {resultado}")
        
        return sequential_chain
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def sequential_chain_avanzada():
    """
    üîó SequentialChain avanzada con m√∫ltiples inputs/outputs
    """
    print_step(4, "SequentialChain Avanzada", "Chains con m√∫ltiples variables")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Chain 1: Analizar concepto
        prompt1 = PromptTemplate(
            input_variables=["concepto"],
            template="""
            Analiza el concepto de programaci√≥n: {concepto}
            
            Proporciona:
            - Definici√≥n: Una definici√≥n clara
            - Ejemplo: Un ejemplo pr√°ctico
            - Dificultad: B√°sico, Intermedio o Avanzado
            """
        )
        chain1 = LLMChain(llm=llm, prompt=prompt1, output_key="analisis")
        
        # Chain 2: Crear ejercicio
        prompt2 = PromptTemplate(
            input_variables=["concepto", "analisis"],
            template="""
            Bas√°ndote en el an√°lisis de {concepto}:
            {analisis}
            
            Crea un ejercicio pr√°ctico para practicar este concepto.
            """
        )
        chain2 = LLMChain(llm=llm, prompt=prompt2, output_key="ejercicio")
        
        # Chain 3: Generar soluci√≥n
        prompt3 = PromptTemplate(
            input_variables=["concepto", "ejercicio"],
            template="""
            Para el concepto {concepto} y el ejercicio:
            {ejercicio}
            
            Proporciona una soluci√≥n paso a paso.
            """
        )
        chain3 = LLMChain(llm=llm, prompt=prompt3, output_key="solucion")

        
        # Crear sequential chain
        sequential_chain = SequentialChain(
            chains=[chain1, chain2, chain3],
            input_variables=["concepto"],
            output_variables=["analisis", "ejercicio", "solucion"],
            verbose=True
        )
        
        # Contexto inicial: {"concepto": "funciones lambda"} (proporcionado por input_variables).
        # Ejecutar chain1: su prompt1 usa input_variables=["concepto"], devuelve texto y lo guarda en el contexto bajo analisis (porque definiste output_key="analisis" en LLMChain).‚Üí Contexto ahora: {"concepto": "...", "analisis": "texto del an√°lisis..."}
        # Ejecutar chain2: su prompt2 declara input_variables=["concepto", "analisis"], por lo tanto toma ambos del contexto, genera ejercicio y lo guarda.‚Üí Contexto ahora incluye "ejercicio": "texto del ejercicio..."
        # Ejecutar chain3: su prompt3 necesita ["concepto", "ejercicio"], genera solucion y la guarda.‚Üí Contexto final: {"concepto": "...", "analisis": "...", "ejercicio": "...", "solucion": "..."}
        # Resultado devuelto: solo las claves listadas en output_variables (aqu√≠ analisis, ejercicio, solucion).
        
        print_result("Sequential Chain Avanzada", sequential_chain)
        
        # Probar la chain
        conceptos = ["funciones lambda", "list comprehensions"]
        
        print("\nüîó Probando sequential chain avanzada:")
        for concepto in conceptos:
            resultado = sequential_chain({"concepto": concepto})
            print(f"\nüìù Concepto: {concepto}")
            print(f"   An√°lisis: {resultado['analisis'][:100]}...")
            print(f"   Ejercicio: {resultado['ejercicio'][:100]}...")
            print(f"   Soluci√≥n: {resultado['solucion'][:100]}...")
        
        return sequential_chain
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def chain_con_output_parser():
    """
    üîß Chain con Output Parser
    """
    print_step(5, "Chain con Output Parser", "Generando respuestas estructuradas")
    
    try:
        from langchain.output_parsers import ResponseSchema, StructuredOutputParser
        
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Definir esquema de respuesta
        response_schemas = [
            ResponseSchema(name="concepto", description="El concepto analizado", type="string"),
            ResponseSchema(name="definicion", description="Definici√≥n clara", type="string"),
            ResponseSchema(name="ejemplo", description="Ejemplo de c√≥digo", type="string"),
            ResponseSchema(name="dificultad", description="Nivel de dificultad", type="string"),
            ResponseSchema(name="aplicaciones", description="Casos de uso comunes", type="string")
        ]
        
        # Crear parser
        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
        
        # Prompt con formato de salida
        prompt = PromptTemplate(
            template="Analiza el concepto de programaci√≥n: {concepto}\n{format_instructions}",
            input_variables=["concepto"],
            partial_variables={"format_instructions": output_parser.get_format_instructions()}
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("Chain con Parser", chain)
        
        # Probar la chain
        conceptos = ["decoradores", "generadores"]
        
        print("\nüîß Probando chain con parser:")
        for concepto in conceptos:
            resultado = chain.run(concepto)
            print(f"\nüìù Concepto: {concepto}")
            print(f"   Resultado: {resultado}")
            
            # Parsear resultado
            try:
                parsed = output_parser.parse(resultado)
                print(f"   Parseado: {parsed}")
            except Exception as e:
                print(f"   Error al parsear: {e}")
        
        return chain, output_parser
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None, None

def chain_con_callbacks():
    """
    üìä Chain con Callbacks para monitoreo
    """
    print_step(6, "Chain con Callbacks", "Monitoreando el rendimiento")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Prompt template
        prompt = PromptTemplate(
            input_variables=["tema"],
            template="Explica el tema de programaci√≥n: {tema}. M√°ximo 3 frases."
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("Chain con Callbacks", chain)
        
        # Usar callbacks para monitoreo
        temas = ["variables", "funciones", "clases"]
        
        print("\nüìä Monitoreando con callbacks:")
        for tema in temas:
            with get_openai_callback() as cb:
                resultado = chain.run(tema)
                
                print(f"\nüìù Tema: {tema}")
                print(f"   Resultado: {resultado}")
                print(f"   Tokens totales: {cb.total_tokens}")
                print(f"   Tokens prompt: {cb.prompt_tokens}")
                print(f"   Tokens respuesta: {cb.completion_tokens}")
                print(f"   Costo: ${cb.total_cost}")
        
        return chain
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def manejo_errores_chains():
    """
    üõ°Ô∏è Manejo de errores en chains
    """
    print_step(7, "Manejo de Errores", "Creando chains robustas")
    
    print("""
    üõ°Ô∏è ESTRATEGIAS DE MANEJO DE ERRORES:
    
    1. üîÑ Retry Logic: Reintentar en caso de fallo
    2. üõ°Ô∏è Try-Catch: Capturar excepciones espec√≠ficas
    3. üîÄ Fallback: Alternativas cuando falla la chain principal
    4. ‚ö†Ô∏è Validation: Validar inputs antes de procesar
    5. üìä Logging: Registrar errores para debugging
    """)
    
    try:
        # Crear LLM con timeout
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7,
            request_timeout=10
        )
        
        # Prompt template
        prompt = PromptTemplate(
            input_variables=["concepto"],
            template="Explica qu√© es {concepto} en programaci√≥n."
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        # Funci√≥n con manejo de errores
        def ejecutar_chain_segura(concepto):
            try:
                resultado = chain.run(concepto)
                return {"success": True, "result": resultado}
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        # Probar con manejo de errores
        conceptos = ["API", "invalid_concept_123", "JSON"]
        
        print("\nüõ°Ô∏è Probando manejo de errores:")
        for concepto in conceptos:
            resultado = ejecutar_chain_segura(concepto)
            if resultado["success"]:
                print(f"‚úÖ {concepto}: {resultado['result']}")
            else:
                print(f"‚ùå {concepto}: Error - {resultado['error']}")
        
        return chain
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def optimizacion_rendimiento():
    """
    ‚ö° Optimizaci√≥n de rendimiento en chains
    """
    print_step(8, "Optimizaci√≥n de Rendimiento", "Mejorando la eficiencia")
    
    print("""
    ‚ö° ESTRATEGIAS DE OPTIMIZACI√ìN:
    
    1. üîÑ Caching: Guardar resultados para reutilizar
    2. ‚ö° Batch Processing: Procesar m√∫ltiples inputs juntos
    3. üéØ Model Selection: Usar modelos apropiados para cada tarea
    4. üìè Token Optimization: Minimizar tokens innecesarios
    5. üîÄ Parallel Processing: Ejecutar chains en paralelo cuando sea posible
    """)
    
    try:
        import time
        
        # Crear LLM optimizado
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",  # Modelo m√°s r√°pido
            temperature=0.7,
            max_tokens=100  # Limitar tokens
        )
        
        # Prompt optimizado
        prompt = PromptTemplate(
            input_variables=["concepto"],
            template="Explica {concepto} brevemente."  # Prompt corto
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        # Medir rendimiento
        conceptos = ["variables", "funciones", "clases", "m√≥dulos"]
        
        print("\n‚ö° Probando optimizaci√≥n:")
        start_time = time.time()
        
        for concepto in conceptos:
            chain_start = time.time()
            resultado = chain.run(concepto)
            chain_time = time.time() - chain_start
            
            print(f"üìù {concepto}: {resultado[:50]}... (Tiempo: {chain_time:.2f}s)")
        
        total_time = time.time() - start_time
        print(f"\n‚è±Ô∏è Tiempo total: {total_time:.2f} segundos")
        print(f"üìä Promedio por concepto: {total_time/len(conceptos):.2f} segundos")
        
        return chain
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return None

def ejercicios_practicos():
    """
    üéØ Ejercicios pr√°cticos
    """
    print_step(9, "Ejercicios Pr√°cticos", "Pon en pr√°ctica lo aprendido")
    
    ejercicios = [
        {
            "titulo": "Chain de An√°lisis de C√≥digo",
            "descripcion": "Crea una chain que analice c√≥digo Python y genere un reporte",
            "objetivo": "Practicar LLMChain con output parser"
        },
        {
            "titulo": "Chain de Traducci√≥n",
            "descripcion": "Crea una sequential chain que traduzca texto y luego lo resuma",
            "objetivo": "Aprender SequentialChain"
        },
        {
            "titulo": "Chain con Validaci√≥n",
            "descripcion": "Crea una chain que valide inputs antes de procesarlos",
            "objetivo": "Practicar manejo de errores"
        },
        {
            "titulo": "Chain de Generaci√≥n de Contenido",
            "descripcion": "Crea una chain que genere contenido educativo paso a paso",
            "objetivo": "Aprender chain composition"
        },
        {
            "titulo": "Chain Optimizada",
            "descripcion": "Optimiza una chain existente para mejor rendimiento",
            "objetivo": "Practicar optimizaci√≥n"
        }
    ]
    
    print("\nüéØ EJERCICIOS PR√ÅCTICOS:")
    for i, ejercicio in enumerate(ejercicios, 1):
        print(f"\n{i}. {ejercicio['titulo']}")
        print(f"   Descripci√≥n: {ejercicio['descripcion']}")
        print(f"   Objetivo: {ejercicio['objetivo']}")

def mejores_practicas():
    """
    ‚≠ê Mejores pr√°cticas para chains
    """
    print_step(10, "Mejores Pr√°cticas", "Consejos para crear chains efectivas")
    
    practicas = [
        "üîó Mant√©n las chains simples y modulares",
        "üìè Usa prompts concisos para optimizar tokens",
        "üõ°Ô∏è Implementa manejo de errores robusto",
        "üìä Monitorea el rendimiento con callbacks",
        "üîÑ Usa caching para respuestas frecuentes",
        "‚ö° Optimiza el modelo seg√∫n la tarea",
        "üîÄ Considera parallel processing cuando sea posible",
        "üìù Documenta tus chains y sus prop√≥sitos",
        "üß™ Prueba tus chains con diferentes inputs",
        "üìà Mide y optimiza continuamente"
    ]
    
    print("\n‚≠ê MEJORES PR√ÅCTICAS:")
    for practica in practicas:
        print(f"   {practica}")

def main():
    """
    üéØ Funci√≥n principal del m√≥dulo
    """
    print_separator("M√ìDULO 4: CHAINS SIMPLES")
    
    # Verificar configuraci√≥n
    if not config.validate_config():
        return
    
    # Contenido del m√≥dulo
    introduccion_chains()
    
    # Chains b√°sicas
    llm_chain_basica()
    llm_chain_con_variables()
    
    # Chains secuenciales
    simple_sequential_chain()
    sequential_chain_avanzada()
    
    # Caracter√≠sticas avanzadas
    chain_con_output_parser()
    chain_con_callbacks()
    
    # Robustez y optimizaci√≥n
    manejo_errores_chains()
    optimizacion_rendimiento()
    
    # Consolidaci√≥n
    mejores_practicas()
    ejercicios_practicos()
    
    print("\nüéâ ¬°M√≥dulo 4 completado! Ahora dominas las chains b√°sicas.")
    print("üöÄ Pr√≥ximo m√≥dulo: Chains Secuenciales Avanzadas")

if __name__ == "__main__":
    main()
