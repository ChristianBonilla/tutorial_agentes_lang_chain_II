"""
ğŸ”— MÃ“DULO 4: CHAINS SIMPLES
===========================

En este mÃ³dulo aprenderÃ¡s a crear y usar chains (cadenas) en LangChain.
Las chains son secuencias de operaciones que combinan diferentes componentes
para crear flujos de procesamiento mÃ¡s complejos.

ğŸ¯ OBJETIVOS DE APRENDIZAJE:
- Entender quÃ© son las chains y cÃ³mo funcionan
- Crear chains simples y secuenciales
- Combinar diferentes tipos de chains
- Optimizar el rendimiento de las chains
- Manejar errores en chains

ğŸ“š CONCEPTOS CLAVE:
- LLMChain
- SequentialChain
- RouterChain
- Chain Composition
- Error Handling
- Performance Optimization

Autor: Tu Instructor de LangChain
Fecha: 2024
"""

import sys
import os
from typing import List, Dict, Any

# Agregar el directorio raÃ­z al path para importar utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.config import config
from utils.helpers import print_separator, print_step, print_result

# Importaciones de LangChain
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain
from langchain.schema import BaseOutputParser
from langchain.callbacks import get_openai_callback

def introduccion_chains():
    """
    ğŸ“ IntroducciÃ³n a las Chains
    """
    print_separator("Â¿QUÃ‰ SON LAS CHAINS?")
    
    print("""
    ğŸ”— Las Chains son el corazÃ³n de LangChain. Son secuencias de operaciones
    que combinan diferentes componentes para crear flujos de procesamiento.
    
    ğŸ—ï¸ ARQUITECTURA DE CHAINS:
    
    Input â†’ [Chain 1] â†’ [Chain 2] â†’ [Chain 3] â†’ Output
    
    ğŸ“‹ TIPOS DE CHAINS:
    
    1. ğŸ”„ LLMChain: La mÃ¡s bÃ¡sica, combina LLM + Prompt
    2. ğŸ”— SequentialChain: Ejecuta chains en secuencia
    3. ğŸ›£ï¸ RouterChain: Dirige el flujo segÃºn condiciones
    4. ğŸ”€ TransformChain: Transforma datos entre chains
    5. ğŸ¯ CustomChain: Chains personalizadas
    
    ğŸ’¡ VENTAJAS:
    - Modularidad y reutilizaciÃ³n
    - Flujos complejos y organizados
    - FÃ¡cil debugging y testing
    - ComposiciÃ³n flexible
    """)

def llm_chain_basica():
    """
    ğŸ”„ LLMChain bÃ¡sica
    """
    print_step(1, "LLMChain BÃ¡sica", "Creando tu primera chain")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7,
            max_tokens=150
        )
        
        # Crear prompt template
        prompt = PromptTemplate(
            input_variables=["concepto"],
            template="Explica quÃ© es {concepto} en el contexto de programaciÃ³n. MÃ¡ximo 2 frases."
        )
        
        # Crear LLMChain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("LLMChain Creada", chain)
        
        # Usar la chain
        conceptos = ["API", "JSON", "REST"]
        
        print("\nğŸ” Probando la chain:")
        for concepto in conceptos:
            resultado = chain.run(concepto)
            print(f"ğŸ“ {concepto}: {resultado}")
        
        return chain
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def llm_chain_con_variables():
    """
    ğŸ”§ LLMChain con mÃºltiples variables
    """
    print_step(2, "LLMChain con Variables", "Chains mÃ¡s complejas")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Prompt con mÃºltiples variables
        prompt = PromptTemplate(
            input_variables=["lenguaje", "concepto", "nivel"],
            template="""
            Eres un instructor de programaciÃ³n experto en {lenguaje}.
            Explica el concepto de {concepto} a nivel {nivel}.
            MantÃ©n la explicaciÃ³n clara y concisa.
            """
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("Chain con Variables", chain)
        
        # Ejemplos de uso
        ejemplos = [
            {"lenguaje": "Python", "concepto": "decoradores", "nivel": "intermedio"},
            {"lenguaje": "JavaScript", "concepto": "closures", "nivel": "avanzado"},
            {"lenguaje": "Java", "concepto": "polimorfismo", "nivel": "bÃ¡sico"}
        ]
        
        print("\nğŸ” Probando con diferentes variables:")
        for ejemplo in ejemplos:
            resultado = chain.run(ejemplo)
            print(f"\nğŸ“ {ejemplo['lenguaje']} - {ejemplo['concepto']} ({ejemplo['nivel']}):")
            print(f"   {resultado}")
        
        return chain
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def simple_sequential_chain():
    """
    ğŸ”— SimpleSequentialChain - Chains en secuencia
    """
    print_step(3, "SimpleSequentialChain", "Ejecutando chains en secuencia")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Chain 1: Generar concepto
        prompt1 = PromptTemplate(
            input_variables=["tema"],
            template="Genera un concepto de programaciÃ³n relacionado con {tema}. Solo el nombre del concepto."
        )
        chain1 = LLMChain(llm=llm, prompt=prompt1)
        
        # Chain 2: Explicar concepto
        prompt2 = PromptTemplate(
            input_variables=["concepto"],
            template="Explica quÃ© es {concepto} en programaciÃ³n. MÃ¡ximo 2 frases."
        )
        chain2 = LLMChain(llm=llm, prompt=prompt2)
        
        # Crear sequential chain
        sequential_chain = SimpleSequentialChain(
            chains=[chain1, chain2],
            verbose=True
        )
        
        print_result("Sequential Chain", sequential_chain)
        
        # Probar la chain
        temas = ["estructuras de datos", "algoritmos", "patrones de diseÃ±o"]
        
        print("\nğŸ”— Probando sequential chain:")
        for tema in temas:
            resultado = sequential_chain.run(tema)
            print(f"\nğŸ“ Tema: {tema}")
            print(f"   Resultado: {resultado}")
        
        return sequential_chain
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def sequential_chain_avanzada():
    """
    ğŸ”— SequentialChain avanzada con mÃºltiples inputs/outputs
    """
    print_step(4, "SequentialChain Avanzada", "Chains con mÃºltiples variables")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Chain 1: Analizar concepto
        prompt1 = PromptTemplate(
            input_variables=["concepto"],
            template="""
            Analiza el concepto de programaciÃ³n: {concepto}
            
            Proporciona:
            - DefiniciÃ³n: Una definiciÃ³n clara
            - Ejemplo: Un ejemplo prÃ¡ctico
            - Dificultad: BÃ¡sico, Intermedio o Avanzado
            """
        )
        chain1 = LLMChain(llm=llm, prompt=prompt1, output_key="analisis")
        
        # Chain 2: Crear ejercicio
        prompt2 = PromptTemplate(
            input_variables=["concepto", "analisis"],
            template="""
            BasÃ¡ndote en el anÃ¡lisis de {concepto}:
            {analisis}
            
            Crea un ejercicio prÃ¡ctico para practicar este concepto.
            """
        )
        chain2 = LLMChain(llm=llm, prompt=prompt2, output_key="ejercicio")
        
        # Chain 3: Generar soluciÃ³n
        prompt3 = PromptTemplate(
            input_variables=["concepto", "ejercicio"],
            template="""
            Para el concepto {concepto} y el ejercicio:
            {ejercicio}
            
            Proporciona una soluciÃ³n paso a paso.
            """
        )
        chain3 = LLMChain(llm=llm, prompt=prompt3, output_key="solucion")

        
        # Crear sequential chain
        sequential_chain = SequentialChain(
            chains=[chain1, chain2, chain3],
            input_variables=["concepto"],
            output_variables=["analisis", "ejercicio", "solucion"],
            verbose=True
        )
        
        # Contexto inicial: {"concepto": "funciones lambda"} (proporcionado por input_variables).
        # Ejecutar chain1: su prompt1 usa input_variables=["concepto"], devuelve texto y lo guarda en el contexto bajo analisis (porque definiste output_key="analisis" en LLMChain).â†’ Contexto ahora: {"concepto": "...", "analisis": "texto del anÃ¡lisis..."}
        # Ejecutar chain2: su prompt2 declara input_variables=["concepto", "analisis"], por lo tanto toma ambos del contexto, genera ejercicio y lo guarda.â†’ Contexto ahora incluye "ejercicio": "texto del ejercicio..."
        # Ejecutar chain3: su prompt3 necesita ["concepto", "ejercicio"], genera solucion y la guarda.â†’ Contexto final: {"concepto": "...", "analisis": "...", "ejercicio": "...", "solucion": "..."}
        # Resultado devuelto: solo las claves listadas en output_variables (aquÃ­ analisis, ejercicio, solucion).
        
        print_result("Sequential Chain Avanzada", sequential_chain)
        
        # Probar la chain
        conceptos = ["funciones lambda", "list comprehensions"]
        
        print("\nğŸ”— Probando sequential chain avanzada:")
        for concepto in conceptos:
            resultado = sequential_chain({"concepto": concepto})
            print(f"\nğŸ“ Concepto: {concepto}")
            print(f"   AnÃ¡lisis: {resultado['analisis'][:100]}...")
            print(f"   Ejercicio: {resultado['ejercicio'][:100]}...")
            print(f"   SoluciÃ³n: {resultado['solucion'][:100]}...")
        
        return sequential_chain
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def chain_con_output_parser():
    """
    ğŸ”§ Chain con Output Parser
    """
    print_step(5, "Chain con Output Parser", "Generando respuestas estructuradas")
    
    try:
        from langchain.output_parsers import ResponseSchema, StructuredOutputParser
        
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Definir esquema de respuesta
        response_schemas = [
            ResponseSchema(name="concepto", description="El concepto analizado", type="string"),
            ResponseSchema(name="definicion", description="DefiniciÃ³n clara", type="string"),
            ResponseSchema(name="ejemplo", description="Ejemplo de cÃ³digo", type="string"),
            ResponseSchema(name="dificultad", description="Nivel de dificultad", type="string"),
            ResponseSchema(name="aplicaciones", description="Casos de uso comunes", type="string")
        ]
        
        # Crear parser
        output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
        
        # Prompt con formato de salida
        prompt = PromptTemplate(
            template="Analiza el concepto de programaciÃ³n: {concepto}\n{format_instructions}",
            input_variables=["concepto"],
            partial_variables={"format_instructions": output_parser.get_format_instructions()}
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("Chain con Parser", chain)
        
        # Probar la chain
        conceptos = ["decoradores", "generadores"]
        
        print("\nğŸ”§ Probando chain con parser:")
        for concepto in conceptos:
            resultado = chain.run(concepto)
            print(f"\nğŸ“ Concepto: {concepto}")
            print(f"   Resultado: {resultado}")
            
            # Parsear resultado
            try:
                parsed = output_parser.parse(resultado)
                print(f"   Parseado: {parsed}")
            except Exception as e:
                print(f"   Error al parsear: {e}")
        
        return chain, output_parser
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None, None

def chain_con_callbacks():
    """
    ğŸ“Š Chain con Callbacks para monitoreo
    """
    print_step(6, "Chain con Callbacks", "Monitoreando el rendimiento")
    
    try:
        # Crear LLM
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7
        )
        
        # Prompt template
        prompt = PromptTemplate(
            input_variables=["tema"],
            template="Explica el tema de programaciÃ³n: {tema}. MÃ¡ximo 3 frases."
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        print_result("Chain con Callbacks", chain)
        
        # Usar callbacks para monitoreo
        temas = ["variables", "funciones", "clases"]
        
        print("\nğŸ“Š Monitoreando con callbacks:")
        for tema in temas:
            with get_openai_callback() as cb:
                resultado = chain.run(tema)
                
                print(f"\nğŸ“ Tema: {tema}")
                print(f"   Resultado: {resultado}")
                print(f"   Tokens totales: {cb.total_tokens}")
                print(f"   Tokens prompt: {cb.prompt_tokens}")
                print(f"   Tokens respuesta: {cb.completion_tokens}")
                print(f"   Costo: ${cb.total_cost}")
        
        return chain
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def manejo_errores_chains():
    """
    ğŸ›¡ï¸ Manejo de errores en chains
    """
    print_step(7, "Manejo de Errores", "Creando chains robustas")
    
    print("""
    ğŸ›¡ï¸ ESTRATEGIAS DE MANEJO DE ERRORES:
    
    1. ğŸ”„ Retry Logic: Reintentar en caso de fallo
    2. ğŸ›¡ï¸ Try-Catch: Capturar excepciones especÃ­ficas
    3. ğŸ”€ Fallback: Alternativas cuando falla la chain principal
    4. âš ï¸ Validation: Validar inputs antes de procesar
    5. ğŸ“Š Logging: Registrar errores para debugging
    """)
    
    try:
        # Crear LLM con timeout
        llm = ChatOpenAI(
            model=config.default_model,
            temperature=0.7,
            request_timeout=10
        )
        
        # Prompt template
        prompt = PromptTemplate(
            input_variables=["concepto"],
            template="Explica quÃ© es {concepto} en programaciÃ³n."
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        # FunciÃ³n con manejo de errores
        def ejecutar_chain_segura(concepto):
            try:
                resultado = chain.run(concepto)
                return {"success": True, "result": resultado}
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        # Probar con manejo de errores
        conceptos = ["API", "invalid_concept_123", "JSON"]
        
        print("\nğŸ›¡ï¸ Probando manejo de errores:")
        for concepto in conceptos:
            resultado = ejecutar_chain_segura(concepto)
            if resultado["success"]:
                print(f"âœ… {concepto}: {resultado['result']}")
            else:
                print(f"âŒ {concepto}: Error - {resultado['error']}")
        
        return chain
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def optimizacion_rendimiento():
    """
    âš¡ OptimizaciÃ³n de rendimiento en chains
    """
    print_step(8, "OptimizaciÃ³n de Rendimiento", "Mejorando la eficiencia")
    
    print("""
    âš¡ ESTRATEGIAS DE OPTIMIZACIÃ“N:
    
    1. ğŸ”„ Caching: Guardar resultados para reutilizar
    2. âš¡ Batch Processing: Procesar mÃºltiples inputs juntos
    3. ğŸ¯ Model Selection: Usar modelos apropiados para cada tarea
    4. ğŸ“ Token Optimization: Minimizar tokens innecesarios
    5. ğŸ”€ Parallel Processing: Ejecutar chains en paralelo cuando sea posible
    """)
    
    try:
        import time
        
        # Crear LLM optimizado
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",  # Modelo mÃ¡s rÃ¡pido
            temperature=0.7,
            max_tokens=100  # Limitar tokens
        )
        
        # Prompt optimizado
        prompt = PromptTemplate(
            input_variables=["concepto"],
            template="Explica {concepto} brevemente."  # Prompt corto
        )
        
        # Crear chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        # Medir rendimiento
        conceptos = ["variables", "funciones", "clases", "mÃ³dulos"]
        
        print("\nâš¡ Probando optimizaciÃ³n:")
        start_time = time.time()
        
        for concepto in conceptos:
            chain_start = time.time()
            resultado = chain.run(concepto)
            chain_time = time.time() - chain_start
            
            print(f"ğŸ“ {concepto}: {resultado[:50]}... (Tiempo: {chain_time:.2f}s)")
        
        total_time = time.time() - start_time
        print(f"\nâ±ï¸ Tiempo total: {total_time:.2f} segundos")
        print(f"ğŸ“Š Promedio por concepto: {total_time/len(conceptos):.2f} segundos")
        
        return chain
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def ejercicios_practicos():
    """
    ğŸ¯ Ejercicios prÃ¡cticos
    """
    print_step(9, "Ejercicios PrÃ¡cticos", "Pon en prÃ¡ctica lo aprendido")
    
    ejercicios = [
        {
            "titulo": "Chain de AnÃ¡lisis de CÃ³digo",
            "descripcion": "Crea una chain que analice cÃ³digo Python y genere un reporte",
            "objetivo": "Practicar LLMChain con output parser"
        },
        {
            "titulo": "Chain de TraducciÃ³n",
            "descripcion": "Crea una sequential chain que traduzca texto y luego lo resuma",
            "objetivo": "Aprender SequentialChain"
        },
        {
            "titulo": "Chain con ValidaciÃ³n",
            "descripcion": "Crea una chain que valide inputs antes de procesarlos",
            "objetivo": "Practicar manejo de errores"
        },
        {
            "titulo": "Chain de GeneraciÃ³n de Contenido",
            "descripcion": "Crea una chain que genere contenido educativo paso a paso",
            "objetivo": "Aprender chain composition"
        },
        {
            "titulo": "Chain Optimizada",
            "descripcion": "Optimiza una chain existente para mejor rendimiento",
            "objetivo": "Practicar optimizaciÃ³n"
        }
    ]
    
    print("\nğŸ¯ EJERCICIOS PRÃCTICOS:")
    for i, ejercicio in enumerate(ejercicios, 1):
        print(f"\n{i}. {ejercicio['titulo']}")
        print(f"   DescripciÃ³n: {ejercicio['descripcion']}")
        print(f"   Objetivo: {ejercicio['objetivo']}")

def mejores_practicas():
    """
    â­ Mejores prÃ¡cticas para chains
    """
    print_step(10, "Mejores PrÃ¡cticas", "Consejos para crear chains efectivas")
    
    practicas = [
        "ğŸ”— MantÃ©n las chains simples y modulares",
        "ğŸ“ Usa prompts concisos para optimizar tokens",
        "ğŸ›¡ï¸ Implementa manejo de errores robusto",
        "ğŸ“Š Monitorea el rendimiento con callbacks",
        "ğŸ”„ Usa caching para respuestas frecuentes",
        "âš¡ Optimiza el modelo segÃºn la tarea",
        "ğŸ”€ Considera parallel processing cuando sea posible",
        "ğŸ“ Documenta tus chains y sus propÃ³sitos",
        "ğŸ§ª Prueba tus chains con diferentes inputs",
        "ğŸ“ˆ Mide y optimiza continuamente"
    ]
    
    print("\nâ­ MEJORES PRÃCTICAS:")
    for practica in practicas:
        print(f"   {practica}")

def main():
    """
    ğŸ¯ FunciÃ³n principal del mÃ³dulo
    """
    print_separator("MÃ“DULO 4: CHAINS SIMPLES")
    
    # Verificar configuraciÃ³n
    if not config.validate_config():
        return
    
    # Contenido del mÃ³dulo
    introduccion_chains()
    
    # Chains bÃ¡sicas
    llm_chain_basica()
    llm_chain_con_variables()
    
    # Chains secuenciales
    simple_sequential_chain()
    sequential_chain_avanzada()
    
    # CaracterÃ­sticas avanzadas
    chain_con_output_parser()
    chain_con_callbacks()
    
    # Robustez y optimizaciÃ³n
    manejo_errores_chains()
    optimizacion_rendimiento()
    
    # ConsolidaciÃ³n
    mejores_practicas()
    ejercicios_practicos()
    
    print("\nğŸ‰ Â¡MÃ³dulo 4 completado! Ahora dominas las chains bÃ¡sicas.")
    print("ğŸš€ PrÃ³ximo mÃ³dulo: Chains Secuenciales Avanzadas")

if __name__ == "__main__":
    main()
